---
title: "R Notebook"
output: html_notebook
---
```{r}
# Loading in necessary packages

library(dplyr)
library(Hmisc)
library(mice)
library(EnvStats)
library(caret)
library(ggformula)
library(car)
library(randomForest)

```

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}

data <- read.csv("music_genre.csv")

data <- na.omit(data)  # five observations containing NAs, removed them

data.clean <- data %>% 
  filter(music_genre %in% c("Rock", "Country")) %>%  # filtering to only include Rock and Country songs
  dplyr::select(-c(1, 2, 3, 16)) %>%  # remove ID, artist name, track name, obtained date
  mutate(key = as.factor(key), mode = as.factor(mode), tempo = as.numeric(tempo), music_genre = as.factor(music_genre)) %>% # convert factor variables and tempo to numeric
  mutate(duration_ms = replace(duration_ms, which(duration_ms < 0), NA)) # replacing negative duration_ms values with NA to be imputed in next step

summary(data.clean)

```


```{r, include=FALSE}

tempData <- mice(data.clean,m=5,maxit=20,meth='pmm',seed=500) # imputing missing data (tempo and duration_ms)

data.clean <- complete(tempData,1) # combining imputed with actual data

```


```{r}

summary(data.clean) #starting to look pretty good

```




```{r}
# exploring histograms of numerical variables

data.clean.num <- select_if(data.clean, is.numeric)

hist.data.frame(data.clean.num)
```


```{r}

# transformations

hist(data.clean$popularity) # Using Original

hist(data.clean$acousticness)
acoustic.box = EnvStats::boxcox(data.clean$acousticness)
bcacoustic = boxcoxTransform(data.clean$acousticness, lambda = acoustic.box$lambda[which.max(acoustic.box$objective)])
hist(bcacoustic) # Using BC Transformation; better than original and log transformation

hist(data.clean$duration_ms) # Using Original

hist(data.clean$energy)
energy.box = EnvStats::boxcox(data.clean$energy)
bcenergy = boxcoxTransform(data.clean$energy, lambda = energy.box$lambda[which.max(energy.box$objective)])
hist(bcenergy) # Using BC Transformation

hist(data.clean$instrumentalness)
hist(log(data.clean$instrumentalness)) # Using log transformation

hist(data.clean$liveness)
hist(log(data.clean$liveness)) # Using log transformation

hist(data.clean$loudness) # Inverse Hyperbolic Sine Transformation for negative values
data.clean <- data.clean %>%
  mutate(loudness = log(loudness+sqrt(loudness^2 +1)))

hist(data.clean$speechiness)
speech.box = EnvStats::boxcox(data.clean$speechiness)
bcspeech = boxcoxTransform(data.clean$speechiness, lambda = speech.box$lambda[which.max(speech.box$objective)])
hist(bcspeech) # Using BC Transformation; better than original  and log transformation

hist(data.clean$tempo) # Using Original

hist(data.clean$valence) # Using Original

```
```{r}
# Replacing original values with transformed values

data.clean <- data.clean %>%
  mutate(acousticness = bcacoustic, energy = bcenergy, liveness = log(liveness), speechiness = bcspeech)

data.clean.num2 <- select_if(data.clean, is.numeric)

hist.data.frame(data.clean.num2)

summary(data.clean)

```


```{r}
# logistic Regression

# Practicing with Log Reg

n = dim(data.clean)[1]
ngroups = 10 # using 10-fold cross-validation
groups = rep(1:ngroups, length = n)

set.seed(123)
cvgroups = sample(groups, n)
all_predicted = numeric(length = n)

for(ii in 1:ngroups){
  groupii = (cvgroups == ii)
  train_set = data.clean[!groupii, ]
  test_set = data.clean[groupii, ]

  model_fit_lr = glm(music_genre ~ ., data = train_set, family = "binomial")

  predicted = predict(model_fit_lr, newdata = test_set, type="response")

  all_predicted[groupii] = predicted
}

table(all_predicted > 0.5, data.clean$music_genre)

```


```{r}
# Random Forest

# Used to determine best mtry value outside of Double CV to save run time

set.seed(88)
data_used = data.clean

ctrl = trainControl(method = "cv", number = 2)
genre_caret = train(music_genre ~ .,
             data = data.clean,
             method = "rf",
             tuneGrid = expand.grid(mtry = c(1:15)),
             trControl = ctrl)

genre_caret

plot(genre_caret)


```

```{r}

##### model assessment OUTER shell #####

# produce loops for 5-fold cross-validation for model ASSESSMENT
n = dim(data.clean)[1]
nfolds = 5
groups = rep(1:nfolds,length=n) #produces list of group labels
set.seed(11)
cvgroups = sample(groups,n) #orders randomly

# set up storage for predicted values from the double-cross-validation
allpredictedCV = rep(NA,n)
# set up storage to see what models are "best" on the inner loops
allbestTypes = rep(NA,nfolds)
allbestPars = vector("list",nfolds)

# loop through outer splits
for (j in 1:nfolds) {
  # j = 1
 groupj = (cvgroups == j)
 traindata = data.clean[!groupj,]
 trainx = model.matrix(music_genre ~ ., data = traindata)[,-1]
 trainy = traindata$music_genre
 validdata = data.clean[groupj,]
 validx = model.matrix(music_genre ~ ., data = validdata)[,-1]
 validy = validdata$music_genre

 #specify data to be used
 dataused=traindata

 # 5-fold Cross Validation via Caret
 
 # Training method
 set.seed(11)
 training = trainControl(method = "cv", number = 5)
 tunegrid <- expand.grid(mtry = c(2, 3, 4, 5))

 # cv for Logistic Regression

 fit_caret_lr = train(music_genre ~ ., data = dataused, method = "glm", family = "binomial", trControl = training)

 # cv for Random Forest
 
 fit_caret_rf = train(music_genre ~ ., data = dataused, method = "rf", tuneGrid = tunegrid, trControl = training)
 
 # All Best

 all_best_type = c("Logistic Regression", "Random Forest")
 all_best_models = list(fit_caret_lr$finalModel, fit_caret_rf$finalModel)
 all_best_accu = c(max(fit_caret_lr$results$Accuracy),max(fit_caret_rf$results$Accuracy))

 # One Best

 one_best_Type = all_best_type[which.max(all_best_accu)]
 one_best_Model = all_best_models[[which.max(all_best_accu)]]

 ##### END OF INNER MODEL SELECTION #####

 allbestTypes[j] = one_best_Type

 if (one_best_Type == "Logistic Regression") { 
  allpredictedCV[groupj] = predict(fit_caret_lr, newdata=validdata, type = "response")
 } else if (one_best_Type == "Random Forest") { 
  allpredictedCV[groupj] = predict(fit_caret_rf, newdata=validdata)
 }
}

# Which models are "best" on each of the inner splits

allbestTypes


```

```{r}
# comparing final results and plot performance

fit_caret_lr$results
fit_caret_rf$results

plot(fit_caret_rf)
plot(fit_caret_rf$finalModel)
legend("topright", 
       colnames(fit_caret_rf$finalModel$err.rate), 
       col = 1:3, lty = 1:3)

table(allpredictedCV, data.clean$music_genre)

((3897 + 4375)/10000)*100

```

```{r}
# Plotting variable importance

varImpPlot(fit_caret_rf$finalModel)


```

```{r}
# Mapping key

data.clean %>%
  gf_bar(~ key, fill =~ music_genre, 
         position = position_dodge())

```

```{r}
# Comparing tempo

data.clean %>%
  gf_boxplot(tempo ~ music_genre)%>%
  gf_labs(title = "Boxplot of Tempo by Genre")

data.clean %>%
  gf_bar(~ mode, fill =~ music_genre, 
         position = position_dodge())%>%
  gf_labs(title = "Boxplot of Mode by Genre")

```

```{r}
# Comparing popularity

data.clean %>%
  gf_boxplot(popularity ~ music_genre) %>%
  gf_labs(title = "Boxplot of Popularity by Genre")

```

```{r}
# Comparing instrumentalness

data.clean %>%
  gf_boxplot(log(instrumentalness) ~ music_genre)%>%
  gf_labs(title = "Boxplot of Instrumentalness by Genre")
```













































































